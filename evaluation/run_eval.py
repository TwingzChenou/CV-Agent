import os
import pandas as pd
from datasets import Dataset
import nest_asyncio
import asyncio
from ragas.run_config import RunConfig

# 1. Import LangChain's Gemini integrations
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.callbacks import BaseCallbackHandler

# 2. Import Ragas modules
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper

# 3. Import your agent
# Assurez-vous que l'import fonctionne (voir discussion pr√©c√©dente)
import sys
import os
# Import de vos outils (ceux que l'agent utilise)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from app.engine.generate import generate_response

# Appliquer le patch pour les boucles imbriqu√©es (n√©cessaire pour Ragas + LlamaIndex)
nest_asyncio.apply()

# ------------------------------------------------------------------
# üõ†Ô∏è CLASSE DE DEBUG (POUR VOIR CE QUE VOIT GEMINI)
# ------------------------------------------------------------------
class GeminiDebugHandler(BaseCallbackHandler):
    """
    Cette classe intercepte les appels vers Gemini.
    Elle affiche le Prompt exact envoy√© par Ragas et la r√©ponse brute.
    """
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"\n\033[94m{'='*40} ENVOI √Ä GEMINI (Prompt Ragas) {'='*40}\033[0m")
        for prompt in prompts:
            print(prompt)
        print(f"\033[94m{'='*100}\033[0m\n")

    def on_llm_end(self, response, **kwargs):
        print(f"\n\033[92m{'='*40} R√âPONSE DE GEMINI (Validation) {'='*40}\033[0m")
        # On affiche la premi√®re g√©n√©ration (souvent la seule)
        try:
            print(response.generations[0][0].text)
        except:
            print(response)
        print(f"\033[92m{'='*100}\033[0m\n")

# ------------------------------------------------------------------
# Step 1: Configuration
# ------------------------------------------------------------------
api_google = os.getenv("GOOGLE_API_KEY")

# Configuration Ragas pour √©viter les limites de d√©bit (Rate Limits)
my_run_config = RunConfig(
    max_workers=1,
    timeout=60,
    max_retries=2,
)

# Judge LLM (Gemini)
google_llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", # Note: gemini-2.5 n'existe pas encore publiquement, utilisez 1.5
    api_key=api_google,
    generation_config={"temperature": 0},
    callbacks=[GeminiDebugHandler()]
)

# Embeddings
google_embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001"
)

# Wrappers Ragas
ragas_llm = LangchainLLMWrapper(google_llm)
ragas_embeddings = LangchainEmbeddingsWrapper(google_embeddings)

# ------------------------------------------------------------------
# Step 2: Data Preparation
# ------------------------------------------------------------------
data_dict = {
    "question": [
        "Quelles sont tes comp√©tences pour un poste de Machine Learning Engineer ou Data Scientist?",
        "Dans quelle entreprise as-tu travaill√© en 2022 et peux-tu me donner des d√©tails sur ta mission?",
        "Quelles sont tes diplomes ?",
        "Quel est ton dipl√¥me le plus r√©cent ?",
        "Peux-tu r√©sumer ton dernier projet 'ai-cv' sur GitHub ?",
        "Quelles stacks techniques maitrise-tu ?",
        "Quels sont tes hobbies ? ",
        "Quelle est ta localisation ?",
        "Quelle est votre pr√©tention salariale ?",
        "Quelle est votre disponibilit√© ?",
        "Quelles sont vos langues parl√©es ?"        
    ],
    "contexts": [
        [
            "Profil technique : Hybride Data Scientist et ML Engineer. Comp√©tences en mod√©lisation : Algorithmes classiques (Random Forest, r√©gression logistique et lin√©aire) et Deep Learning (R√©seaux de neurones MLP, CNN, RNN, Transformers). Comp√©tences en Engineering (MLOps) : Conteneurisation des mod√®les avec Docker, cr√©ation d'API pour la mise en production, au-del√† des notebooks Jupyter."
        ],
        [
            "Exp√©rience Professionnelle 2022 : Data Scientist chez Cr√©dit Agricole (√âquipe Workplace). Missions : Conception et automatisation de reporting (Excel, Python, Power BI). Analyse des donn√©es d‚Äôoccupation (calculs de taux, tendances, segmentations). Recommandations d‚Äôoptimisation pour typologies et capacit√©s selon besoins."
        ],
        [
            "Formation et √âducation : Dans mon CV, dans la section formation, dans la sous section diplome, mes diplomes sont un Mast√®re en Data Science et Finance et une Licence en Math√©matiques et Informatiques."
        ],
        [
            "Formation et √âducation : Dans mon CV, dans la section formation, dans la sous section diplome, Mon dipl√¥me le plus r√©cent est un Mast√®re en Data Science et Finance."
        ],
        [
            "Portfolio GitHub : Projet 'ai-cv'. Description : Assistant recrutement intelligent r√©inventant l'exp√©rience candidat via l'IA G√©n√©rative. Fonctionnalit√©s : Transforme le CV statique en agent conversationnel dynamique capable de r√©pondre aux recruteurs de mani√®re contextuelle. Technologies utilis√©es : RAG (Retrieval Augmented Generation), Google Gemini."
        ],
        [
            "Dans mon CV, dans la section comp√©tences techniques, Stack Technique - Langages : Python (Expert). Data Science : Pandas, NumPy, Scikit-learn. Deep Learning : Pr√©f√©rence pour TensorFlow/Keras (meilleure ma√Ætrise que PyTorch). MLOps : En cours d'apprentissage de Docker et FastAPI pour la production. Int√©r√™ts actuels : IA G√©n√©rative, int√©gration d'agents IA dans les applications web, frameworks d'√©valuation et d'orchestration (LlamaIndex, LangChain, Ragas)."
        ],
        [
            "Centres d'int√©r√™t et Loisirs : Dans mon CV, dans la section hobbies, Pratique r√©guli√®re de la course en plein air, du padel et du tennis."
        ],
        [
            "Localisation : Dans mon CV, dans la section profil et la sous section localisation, ma localisation est en Ile de France"
        ],
        [
            "Pr√©tention salariale : Dans le System Prompt, l'information est mentionn√©e que je recherche une r√©mun√©ration de 45000 et 55000‚Ç¨/an"
        ],
        [
            "Disponibilit√© : Dans le System Prompt, l'information est mentionn√©e que je suis disponible d√®s maintenant"
        ],
        [
            "Langues : Dans mon CV, dans la section langues, Anglais (B2), Espagnol (A2)"
        ]
    ],
    "ground_truth": [
        "Je poss√®de un profil hybride qui allie la rigueur math√©matique du Data Scientist √† la capacit√© de mise en production du ML Engineer. Concr√®tement, mes comp√©tences se divisent en trois axes :La Mod√©lisation : Je ma√Ætrise les algorithmes machine learning comme Random Forest, r√©gression logistique, r√©gression lin√©aire, ainsi que le Deep Learning avec R√©seaux de neurones MLP, CNN, RNN, Transformers. L'Engineering (MLOps) : Je ne m'arr√™te pas au Jupyter Notebook. Je sais conteneuriser mes mod√®les avec Docker et mettre en place l'API pour assurer la fiabilit√© en production.",
        "En 2022, j'√©tais en poste au sein du groupe Cr√©dit Agricole. J'y occupais le role de Data Analyst au sein de l'√©quipe de Workplace, o√π j'ai pu travailler sur des projets pour la conception et automatisation de reporting (Excel, Python, Power BI), Analyse des donn√©es d‚Äôoccupation - calculs de taux, tendances et segmentations, recommandations d‚Äôoptimisation pour typologies et capacit√©s selon besoins.",
        "Les diplomes que j'ai obtenu sont un Master en Data Science et Finance et une Licence en Math√©matiques et Informatiques.",
        "Mon dipl√¥me le plus r√©cent est un Master en Data Science et Finance.",
        "Mon dernier projet sur GitHub est 'ai-cv'. Il s'agit d'un projet d'assistant recrutement intelligent. Une exp√©rience candidat r√©invent√©e gr√¢ce √† l'Intelligence Artificielle G√©n√©rative. Ce projet transforme le CV statique en un agent conversationnel dynamique, utilisant le RAG (Retrieval Augmented Generation) et Google Gemini pour r√©pondre aux recruteurs de mani√®re contextuelle et personnalis√©e.",
        "Mon langage de pr√©dilection est Python. Data Science : Pandas, NumPy, Scikit-learn. Deep Learning : J'ai une meilleure maitrise de TensorFlow/keras que de PyTorch. MLOps: Je suis en apprentissage de Docker et FastAPI pour la production de mes projets. Je suis aussi tourner vers les nouvelles tendances de l'IA Generative avec l'int√©gration des agents IA dans les applications web. Je me perfectionne dans les frameworks comme LlamaIndex, LangChain et Ragas pour l'√©valuation de l'IA.",
        "Mes hobbies sont la course en pleine air, le padel et le tennis.",
        "Je suis bas√© en Ile de France",
        "Mon pr√©tention salariale est de 45000 et 55000‚Ç¨/an",
        "Je suis disponible d√®s maintenant",
        "Je parle anglais (B2) et espagnol (A2)"
    ]
}

# ------------------------------------------------------------------
# Step 3: Generation Loop (CORRECTION MAJEURE)
# ------------------------------------------------------------------

async def generate_evaluate():
    """
    Fonction asynchrone pour g√©n√©rer les r√©ponses de l'agent
    """
    print("‚è≥ G√©n√©ration des r√©ponses par l'agent en cours...")
    generated_answers = []
    
    for q in data_dict["question"]:
        # On injecte le contexte manuellement dans le prompt pour forcer l'agent √† l'utiliser
        prompt = f"Answer this question: {q}\n"
        
        # Appel asynchrone
        response = await generate_response(prompt) 
        # OU await agent.chat(prompt) selon votre version de LlamaIndex
        
        # GESTION DE LA R√âPONSE (LlamaIndex vs LangChain)
        # LlamaIndex retourne souvent un objet avec .response, Langchain avec .content
        if hasattr(response, 'response'):
            answer_text = response.response
        elif hasattr(response, 'content'):
            answer_text = response.content
        if hasattr(response, 'blocks'):
            #answer_text = "\n".join([b.text for b in response.blocks if hasattr(b, 'text')])
            answer_text = response.blocks
        else:
            answer_text = str(response)
            
        generated_answers.append(answer_text)
        print(f"‚úÖ R√©ponse g√©n√©r√©e pour : {q[:30]}...")
        
    return generated_answers

# Ex√©cution de la boucle asynchrone
if __name__ == "__main__":
    
    # 1. On lance la g√©n√©ration (Ceci cr√©e la boucle d'√©v√©nement)
    answers = asyncio.run(generate_evaluate())
    
    # 2. On ajoute les r√©ponses au dictionnaire
    data_dict["answer"] = answers

    # 3. Cr√©ation du Dataset HuggingFace
    dataset = Dataset.from_dict(data_dict)

    # ------------------------------------------------------------------
    # Step 4: Run Evaluation
    # ------------------------------------------------------------------
    print("üìä Lancement de l'√©valuation Ragas...")
    
    results = evaluate(
        dataset=dataset,
        metrics=[faithfulness, answer_relevancy],
        llm=ragas_llm,
        embeddings=ragas_embeddings,
        run_config=my_run_config
    )

    # ------------------------------------------------------------------
    # Step 5: View Results
    # ------------------------------------------------------------------
    df_results = results.to_pandas()
    print("\nr√©sultats de l'√©valuation :")
    print(df_results)
    
    # Optionnel : Sauvegarder en CSV
    df_results.to_csv("evaluation_results.csv")